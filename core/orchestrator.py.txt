import asyncio
import re
import aiohttp
from typing import List, Dict, Any, AsyncGenerator

from .models import create_model_instance
import core.database as db

async def _get_completed_future(result):
    """辅助函数：将结果包装成已完成的future"""
    return result

class Orchestrator:
    """核心编排器：管理多模型协作的完整流程"""
    
    async def process_query_stream(self, user_question: str, selected_models: List[str], 
                                   history: List[Dict[str, str]]) -> AsyncGenerator[Dict[str, Any], None]:
        """流式处理用户查询的主函数"""
        yield {"type": "status", "data": "初始化模型..."}
        
        # 创建模型实例
        active_models = [
            instance for sm_id in selected_models
            if (provider_config := db.get_provider_by_name(sm_id.split('::', 1)[0]))
            and (instance := create_model_instance(provider_config, sm_id.split('::', 1)[1]))
        ]
        
        if not active_models:
            yield {"type": "error", "data": "错误：没有选择任何有效的模型。"}
            return
        
        messages = history + [{"role": "user", "content": user_question}]
        
        async with aiohttp.ClientSession() as session:
            # 第一轮：生成初始答案
            yield {"type": "status", "data": f"第一轮：{len(active_models)}个模型正在生成初始答案..."}
            initial_answers = await self._generate_initial_answers(messages, active_models, session)
            
            # 如果只有一个模型，跳过评审环节
            if len(active_models) > 1:
                # 第二轮：结构化评审
                yield {"type": "status", "data": "第二轮：使用统一标准进行结构化评审..."}
                critiques = await self._structured_critique(user_question, initial_answers, active_models, session)
                
                # 第三轮：修正答案
                yield {"type": "status", "data": "第三轮：根据评审修正答案..."}
                revised_answers = await self._revise_answers(initial_answers, critiques, active_models, session)
            else:
                critiques, revised_answers = {}, initial_answers
            
            # 最终决策
            yield {"type": "status", "data": "最终决策..."}
            final_decision, all_results = self._make_final_decision(initial_answers, critiques, revised_answers)
            yield {"type": "final_result", "data": {"best_answer": final_decision, "process_details": all_results}}
    
    async def _generate_initial_answers(self, messages: List[Dict], models: List, session: aiohttp.ClientSession) -> Dict[str, str]:
        """第一轮：并发生成所有模型的初始答案"""
        tasks = [model.generate(messages, session) for model in models]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return {
            models[i].name: str(res) if not isinstance(res, Exception) else f"API调用失败: {res}" 
            for i, res in enumerate(results)
        }
    
    async def _structured_critique(self, question: str, answers: Dict[str, str], 
                                   models: List, session: aiohttp.ClientSession) -> Dict[str, List[Dict]]:
        """第二轮：使用结构化Rubric进行评审"""
        pure_name_map = {m.name: m.name.split('::', 1)[1] for m in models}
        reverse_name_map = {v: k for k, v in pure_name_map.items()}
        tasks = []
        
        for critic in models:
            # 构建其他模型的答案文本
            others_text = "".join([
                f"\n\n=== 模型 '{pure_name_map.get(n, n)}' 的答案 ===\n{a}" 
                for n, a in answers.items() if n != critic.name
            ])
            if not others_text.strip():
                continue
            
            prompt = self._build_structured_critique_prompt(question, others_text)
            tasks.append((critic.name, critic.generate([{"role": "user", "content": prompt}], session)))
        
        if not tasks:
            return {name: [] for name in answers.keys()}
        
        results = await asyncio.gather(*[t for _, t in tasks], return_exceptions=True)
        all_critiques = {name: [] for name in answers.keys()}
        
        # 解析评审结果
        for i, (critic_name, _) in enumerate(tasks):
            text = str(results[i]) if not isinstance(results[i], Exception) else f"评审失败:{results[i]}"
            for critique in self._parse_structured_critique(text, critic_name, reverse_name_map):
                if critique.get('model_name') in all_critiques:
                    all_critiques[critique['model_name']].append(critique)
        
        return all_critiques
    
    async def _revise_answers(self, initial_answers: Dict[str, str], critiques: Dict[str, List[Dict]], 
                             models: List, session: aiohttp.ClientSession) -> Dict[str, str]:
        """第三轮：根据评审修正答案"""
        tasks = []
        for model in models:
            my_critiques = critiques.get(model.name, [])
            if not my_critiques:
                # 没有收到评审，保持原答案
                tasks.append((model.name, _get_completed_future(initial_answers.get(model.name, ""))))
            else:
                # 根据评审生成修正版本
                prompt = self._build_revision_prompt(initial_answers.get(model.name, ""), my_critiques)
                tasks.append((model.name, model.generate([{"role": "user", "content": prompt}], session)))
        
        results = await asyncio.gather(*[t for _, t in tasks], return_exceptions=True)
        return {
            name: str(res) if not isinstance(res, Exception) else f"修正失败:{res}" 
            for (name, _), res in zip(tasks, results)
        }
    
    def _make_final_decision(self, initial: Dict[str, str], critiques: Dict[str, List[Dict]], 
                            revised: Dict[str, str]) -> tuple:
        """最终决策：基于结构化评分选择最佳答案"""
        scores = {}
        for model_name, critique_list in critiques.items():
            total_score = sum(c.get('score', 0) for c in critique_list)
            count = len(critique_list)
            scores[model_name] = total_score / count if count > 0 else 0
        
        results = []
        for model_name in initial.keys():
            results.append({
                "model_name": model_name,
                "initial_answer": initial.get(model_name, "N/A"),
                "critiques_received": critiques.get(model_name, []),
                "revised_answer": revised.get(model_name, "N/A"),
                "total_score": scores.get(model_name, 0)
            })
        
        results.sort(key=lambda x: x['total_score'], reverse=True)
        
        if not results:
            return "无可用结果", []
        
        best = results[0]
        return best.get('revised_answer') or best.get('initial_answer') or "未能生成最终答案", results
    
    def _build_structured_critique_prompt(self, question: str, other_answers: str) -> str:
        """构建基于Rubric的结构化评审提示词"""
        return f"""You are an expert AI evaluator. Your task is to assess answers from other AI models using a STANDARDIZED RUBRIC.

**Original Question:** "{question}"

**Answers to Evaluate:**
{other_answers}

---
**EVALUATION RUBRIC (Score each dimension from 0-3):**

1. **Accuracy (准确性)**: 
   - 0: Contains factual errors or logical fallacies
   - 1: Mostly correct but has minor inaccuracies
   - 2: Accurate and complete information
   - 3: Highly accurate with additional correct insights

2. **Completeness (完整性)**:
   - 0: Doesn't answer the question or evades it
   - 1: Partially answers, missing key aspects
   - 2: Addresses all main aspects of the question
   - 3: Comprehensive, explores related extensions

3. **Clarity (清晰性)**:
   - 0: Confusing or incoherent
   - 1: Understandable but has ambiguities
   - 2: Clear and well-structured
   - 3: Exceptionally clear and elegant

4. **Usefulness (实用性)**:
   - 0: Not helpful to the user
   - 1: Somewhat helpful but not actionable
   - 2: Practical and directly applicable
   - 3: Highly practical with optimization tips

---
**OUTPUT FORMAT (Use '###' to separate evaluations):**

###
模型名称: [exact model name, e.g., 'gpt-4']
准确性: [0-3]
完整性: [0-3]
清晰性: [0-3]
实用性: [0-3]
总分: [sum of above, max 12]
评语: [A brief summary explaining your scores, focusing on specific strengths and concrete improvement suggestions]
###

**IMPORTANT:** Evaluate ALL models using the SAME standards. Be objective and consistent."""

    def _build_revision_prompt(self, my_initial_answer: str, my_critiques: List[Dict]) -> str:
        """构建修正提示词"""
        feedback_text = "".join([
            f"""---
评审员: {c.get('critic_name', 'Anonymous')}
总分: {c.get('score', 'N/A')}/12

维度得分:
- 准确性: {c.get('accuracy', 'N/A')}/3
- 完整性: {c.get('completeness', 'N/A')}/3
- 清晰性: {c.get('clarity', 'N/A')}/3
- 实用性: {c.get('usefulness', 'N/A')}/3

评语: {c.get('comment', 'N/A')}
"""
            for c in my_critiques
        ])

        return f"""You are revising your answer based on structured peer feedback.

**Your Original Answer:**
---
{my_initial_answer}
---

**Feedback Received:**
{feedback_text}
---

**Your Task:**
Carefully analyze the feedback using the 4-dimension rubric (Accuracy, Completeness, Clarity, Usefulness).
Address the identified weaknesses and integrate valid suggestions.
Output ONLY the final, improved answer without any preamble or meta-commentary."""

    def _parse_structured_critique(self, critique_text: str, critic_name: str, 
                                   reverse_name_map: Dict[str, str]) -> List[Dict]:
        """解析结构化评审文本"""
        critiques = []
        pattern = re.compile(r"^\s*模型名称\s*[:：]\s*(.*?)\s*$", re.M | re.I)
        
        for block in critique_text.split('###'):
            if not block.strip():
                continue
            
            # 提取模型名称
            name_match = pattern.search(block)
            if not name_match:
                continue
            
            pure_name = name_match.group(1).strip().strip("'\"")
            full_model_id = reverse_name_map.get(pure_name)
            if not full_model_id:
                continue
            
            # 初始化数据结构
            data = {
                "critic_name": critic_name,
                "model_name": full_model_id,
                "accuracy": 0,
                "completeness": 0,
                "clarity": 0,
                "usefulness": 0,
                "score": 0,
                "comment": "N/A"
            }
            
            # 提取各维度分数
            accuracy_match = re.search(r"准确性\s*[:：]\s*(\d+)", block, re.I)
            completeness_match = re.search(r"完整性\s*[:：]\s*(\d+)", block, re.I)
            clarity_match = re.search(r"清晰性\s*[:：]\s*(\d+)", block, re.I)
            usefulness_match = re.search(r"实用性\s*[:：]\s*(\d+)", block, re.I)
            total_match = re.search(r"总分\s*[:：]\s*(\d+)", block, re.I)
            comment_match = re.search(r"评语\s*[:：]([\s\S]*?)(?=###|$)", block, re.I)
            
            if accuracy_match:
                data["accuracy"] = int(accuracy_match.group(1))
            if completeness_match:
                data["completeness"] = int(completeness_match.group(1))
            if clarity_match:
                data["clarity"] = int(clarity_match.group(1))
            if usefulness_match:
                data["usefulness"] = int(usefulness_match.group(1))
            if total_match:
                data["score"] = int(total_match.group(1))
            else:
                # 如果没有总分，自动计算
                data["score"] = data["accuracy"] + data["completeness"] + data["clarity"] + data["usefulness"]
            if comment_match:
                data["comment"] = comment_match.group(1).strip()
            
            critiques.append(data)
        
        return critiques
