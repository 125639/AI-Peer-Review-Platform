import asyncio
import re
from typing import List, Dict, Any, AsyncGenerator, Optional
from .models import create_model_instance
import core.database as db

class Orchestrator:
    async def process_query_stream(self, user_question: str, selected_models: List[str], 
                                   history: List[Dict[str, str]], ocr_text: Optional[str] = None) -> AsyncGenerator[Dict[str, Any], None]:
        yield {"type": "status", "data": "æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹..."}
        
        active_models = []
        for sm_id in selected_models:
            parts = sm_id.split('::', 1)
            if len(parts) != 2:
                continue
            provider_name, model_name = parts
            provider_config = db.get_provider_by_name(provider_name)
            if provider_config:
                if instance := create_model_instance(provider_config, model_name):
                    active_models.append(instance)
        
        if not active_models:
            yield {"type": "error", "data": "æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹"}
            return
        
        # å¦‚æœæœ‰OCRæ–‡æœ¬ï¼Œå°†å…¶ä¸ç”¨æˆ·é—®é¢˜åˆå¹¶ï¼Œä½œä¸ºæ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡
        if ocr_text and ocr_text.strip():
            combined_question = (
                f"ä»¥ä¸‹æ˜¯å›¾ç‰‡è¯†åˆ«å¾—åˆ°çš„æ–‡å­—å†…å®¹ï¼š\n{ocr_text.strip()}\n\n"
                f"è¯·ç»“åˆä»¥ä¸Šè¯†åˆ«æ–‡æœ¬å›ç­”ï¼š{user_question}"
            )
        else:
            combined_question = user_question

        messages = history + [{"role": "user", "content": combined_question}]
        
        yield {"type": "status", "data": "ç¬¬ä¸€è½®ï¼šç”Ÿæˆåˆå§‹ç­”æ¡ˆ..."}
        
        initial_answers = {}
        results = await asyncio.gather(*[model.generate(messages) for model in active_models], return_exceptions=True)
        
        for model, result in zip(active_models, results):
            initial_answers[model.name] = f"[å¤±è´¥: {result}]" if isinstance(result, Exception) else result
            yield {"type": "initial_answer_complete", "model_name": model.name, "answer": initial_answers[model.name]}
        
        if len(active_models) == 1:
            single_model_name = active_models[0].name
            yield {
                "type": "final_result",
                "data": {
                    "best_answer": initial_answers[single_model_name],
                    "process_details": [{
                        "model_name": single_model_name,
                        "initial_answer": initial_answers[single_model_name],
                        "critiques_received": [],
                        "revised_answer": initial_answers[single_model_name],
                        "total_score": 0
                    }]
                }
            }
            return
        
        yield {"type": "status", "data": "ç¬¬äºŒè½®ï¼šäº’ç›¸è¯„å®¡..."}
        
        critiques = {m.name: [] for m in active_models}
        critique_tasks = [
            (critic.name, target.name, self._generate_critique(critic, target.name, user_question, initial_answers.get(target.name, "")))
            for critic in active_models for target in active_models if critic.name != target.name
        ]
        
        results = await asyncio.gather(*[task for _, _, task in critique_tasks], return_exceptions=True)
        
        for (critic_name, target_name, _), result in zip(critique_tasks, results):
            if not isinstance(result, Exception):
                critique_text, parsed = result
                critiques[target_name].append(parsed)
                yield {
                    "type": "critique_complete",
                    "critic_name": critic_name,
                    "target_model": target_name,
                    "critique_text": critique_text,
                    "critique_data": parsed
                }
        
        yield {"type": "status", "data": "ç¬¬ä¸‰è½®ï¼šæ”¹è¿›ç­”æ¡ˆ..."}
        
        revised_answers = {}
        revision_tasks = [
            (model.name, self._generate_revision(model, initial_answers.get(model.name, ""), critiques.get(model.name, [])))
            for model in active_models if critiques.get(model.name)
        ]
        
        if revision_tasks:
            results = await asyncio.gather(*[task for _, task in revision_tasks], return_exceptions=True)
            for (model_name, _), result in zip(revision_tasks, results):
                revised_answers[model_name] = initial_answers.get(model_name, "") if isinstance(result, Exception) else result
                yield {"type": "revision_complete", "model_name": model_name, "revised_answer": revised_answers[model_name]}
        
        for model in active_models:
            if model.name not in revised_answers:
                revised_answers[model.name] = initial_answers.get(model.name, "")
        
        yield {"type": "status", "data": "æœ€ç»ˆå†³ç­–..."}
        best_answer, details = self._make_final_decision(initial_answers, critiques, revised_answers)
        yield {"type": "final_result", "data": {"best_answer": best_answer, "process_details": details}}
    
    async def _generate_critique(self, critic_model, target_name: str, question: str, answer: str) -> tuple:
        active_prompt = db.get_active_prompt()
        prompt = self._build_critique_prompt(question, target_name, answer, active_prompt)

        attempts = 0
        max_attempts = 3
        critique_text = ""
        parsed = {}

        while attempts < max_attempts:
            attempts += 1

            # é¦–æ¬¡å°è¯•ä½¿ç”¨åŸå§‹æç¤ºè¯ï¼Œåç»­å°è¯•åœ¨æç¤ºè¯ä¸­è¿½åŠ çº æ­£è¯´æ˜
            if attempts == 1:
                current_prompt = prompt
            else:
                current_prompt = self._build_retry_prompt(
                    question=question,
                    target=target_name,
                    answer=answer,
                    previous_output=critique_text,
                    missing_fields=parsed.get("missing_fields", []),
                    attempt=attempts
                )

            critique_text = await critic_model.generate([
                {"role": "user", "content": current_prompt}
            ])

            parsed = self._parse_critique(critique_text, critic_model.name)

            if not parsed.get("missing_fields") and parsed.get("comment"):
                break

        if parsed.get("missing_fields"):
            missing_display = "ã€".join(parsed["missing_fields"])
            print(
                f"âš ï¸ {critic_model.name} åœ¨ {attempts} æ¬¡å°è¯•åä»ç¼ºå°‘å­—æ®µ: {missing_display}. "
                "å°†ä½¿ç”¨å½“å‰è§£æç»“æœç»§ç»­æµç¨‹ã€‚"
            )

        return (critique_text, parsed)
    
    async def _generate_revision(self, model, original: str, critiques: List[Dict]) -> str:
        active_prompt = db.get_active_prompt()
        prompt = self._build_revision_prompt(original, critiques, active_prompt)
        return await model.generate([{"role": "user", "content": prompt}])
    
    def _make_final_decision(self, initial: Dict, critiques: Dict, revised: Dict):
        scores = {}
        for name, clist in critiques.items():
            # è¿‡æ»¤æ‰æ— æ•ˆçš„ã€å¸¦æœ‰é”™è¯¯çš„è¯„å®¡
            valid_critiques = [c for c in clist if not c.get("error")]
            if valid_critiques:
                scores[name] = sum(c.get('score', 0) for c in valid_critiques) / len(valid_critiques)
            else:
                scores[name] = 0
        
        results = [
            {
                "model_name": name,
                "initial_answer": initial.get(name, ""),
                "critiques_received": critiques.get(name, []),
                "revised_answer": revised.get(name, initial.get(name, "")),
                "total_score": scores.get(name, 0)
            }
            for name in initial.keys()
        ]
        
        results.sort(key=lambda x: x['total_score'], reverse=True)
        best_answer = results[0].get('revised_answer', '') if results else "æ— ç»“æœ"
        return best_answer, results
    
    def _build_critique_prompt(self, question: str, target: str, answer: str, prompt_template: Optional[Dict] = None) -> str:
        if prompt_template and prompt_template.get('critique_prompt'):
            template = prompt_template['critique_prompt']
            return template.format(question=question, target=target, answer=answer)
        # é»˜è®¤æç¤ºè¯
        return f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒè¡Œè¯„å®¡ä¸“å®¶ã€‚ã€é‡è¦ï¼šå¿…é¡»ä¸¥æ ¼æŒ‰ç…§æŒ‡å®šæ ¼å¼è¾“å‡ºï¼Œå¦åˆ™è¯„å®¡æ— æ•ˆã€‘

ã€è¯„å®¡ä»»åŠ¡ã€‘
é—®é¢˜: {question}
è¢«è¯„å®¡æ¨¡å‹: {target}
è¢«è¯„å®¡ç­”æ¡ˆ: {answer}

ã€è¯„åˆ†æ ‡å‡†ã€‘(æ¯é¡¹0-3åˆ†)
1. å‡†ç¡®æ€§: 3=å®Œå…¨å‡†ç¡® 2=åŸºæœ¬å‡†ç¡® 1=æœ‰é”™è¯¯ 0=ä¸¥é‡é”™è¯¯
2. å®Œæ•´æ€§: 3=å…¨é¢è¦†ç›– 2=ç•¥æœ‰é—æ¼ 1=æ˜æ˜¾ä¸è¶³ 0=ä¸¥é‡ä¸å®Œæ•´
3. æ¸…æ™°æ€§: 3=æ¸…æ™°æ˜äº† 2=åŸºæœ¬æ¸…æ™° 1=è¡¨è¾¾æ··ä¹± 0=éš¾ä»¥ç†è§£
4. å®ç”¨æ€§: 3=éå¸¸å®ç”¨ 2=æœ‰å¸®åŠ© 1=ç¼ºä¹å®æ“æ€§ 0=æ— ç”¨

ã€è¾“å‡ºæ ¼å¼ - å¿…é¡»ä¸¥æ ¼éµå®ˆã€‘
è¯·ç›´æ¥è¾“å‡ºä»¥ä¸‹æ ¼å¼ï¼Œä¸è¦æ·»åŠ ä»»ä½•å‰ç¼€ã€æ ‡é¢˜æˆ–é¢å¤–å†…å®¹ï¼š

å‡†ç¡®æ€§: [å†™0æˆ–1æˆ–2æˆ–3]
å®Œæ•´æ€§: [å†™0æˆ–1æˆ–2æˆ–3]
æ¸…æ™°æ€§: [å†™0æˆ–1æˆ–2æˆ–3]
å®ç”¨æ€§: [å†™0æˆ–1æˆ–2æˆ–3]
æ€»åˆ†: [å››é¡¹å¾—åˆ†ç›¸åŠ ï¼Œ0-12ä¹‹é—´]
è¯„è¯­: [è‡³å°‘50å­—çš„è¯¦ç»†è¯„è¯­ï¼Œå¿…é¡»åŒ…æ‹¬ï¼š1)å…·ä½“ä¼˜ç‚¹ 2)æ˜ç¡®ç¼ºé™· 3)æ”¹è¿›å»ºè®®]

ã€ç¤ºä¾‹è¾“å‡ºã€‘
å‡†ç¡®æ€§: 2
å®Œæ•´æ€§: 2
æ¸…æ™°æ€§: 3
å®ç”¨æ€§: 1
æ€»åˆ†: 8
è¯„è¯­: è¯¥ç­”æ¡ˆå‡†ç¡®æ€§è¾ƒå¥½ï¼ŒåŸºæœ¬ç¬¦åˆäº‹å®ã€‚å®Œæ•´æ€§æ–¹é¢ç•¥æœ‰ä¸è¶³ï¼Œç¼ºå°‘äº†å¯¹Xçš„è®¨è®ºã€‚è¡¨è¾¾æ¸…æ™°æ˜“æ‡‚ã€‚ä½†å®ç”¨æ€§è¾ƒå·®ï¼Œç¼ºä¹å…·ä½“çš„æ“ä½œæ­¥éª¤ã€‚å»ºè®®è¡¥å……å®é™…æ¡ˆä¾‹å’Œè¯¦ç»†æ­¥éª¤ï¼Œå¢åŠ ä»£ç ç¤ºä¾‹ã€‚

ã€è­¦å‘Šã€‘
- ä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–æ ¼å¼
- ä¸è¦çœç•¥ä»»ä½•è¯„åˆ†é¡¹
- è¯„è¯­ä¸èƒ½å°‘äº50å­—
- å¿…é¡»ç»™å‡ºå…·ä½“çš„æ”¹è¿›å»ºè®®"""

    def _build_retry_prompt(
        self,
        question: str,
        target: str,
        answer: str,
        previous_output: str,
        missing_fields: List[str],
        attempt: int
    ) -> str:
        """åœ¨è¯„å®¡è¾“å‡ºç¼ºå°‘è¯„åˆ†å­—æ®µæ—¶ï¼Œæ„é€ è¡¥æ•‘æç¤ºè¯é‡æ–°è¯·æ±‚æ¨¡å‹è¾“å‡ºã€‚"""

        missing_map = {
            "accuracy": "å‡†ç¡®æ€§",
            "completeness": "å®Œæ•´æ€§",
            "clarity": "æ¸…æ™°æ€§",
            "usefulness": "å®ç”¨æ€§",
            "total": "æ€»åˆ†",
            "comment": "è¯„è¯­"
        }

        missing_display = "ã€".join(missing_map.get(field, field) for field in missing_fields)
        base_prompt = self._build_critique_prompt(question, target, answer, db.get_active_prompt())

        retry_instruction = (
            f"âš ï¸ ç¬¬ {attempt} æ¬¡å°è¯•ï¼šä½ ä¹‹å‰çš„å›ç­”ç¼ºå°‘ä»¥ä¸‹å­—æ®µ: {missing_display}ã€‚"
            "è¯·ä¸¥æ ¼æŒ‰ç…§æ ¼å¼é‡æ–°ç»™å‡ºè¯„å®¡ç»“æœã€‚åŠ¡å¿…ä½¿ç”¨é˜¿æ‹‰ä¼¯æ•°å­— (0-3) å¡«å†™æ¯ä¸€é¡¹ï¼Œå¹¶æä¾›ä¸å°‘äº50å­—çš„è¯„è¯­ã€‚"
            "ä¸è¦å¤è¿°ä¸Šæ¬¡çš„ç­”æ¡ˆï¼Œä¹Ÿä¸è¦åŠ å…¥ä»»ä½•è¯´æ˜æ€§æ–‡å­—ã€‚ç›´æ¥è¾“å‡ºæ ¼å¼åŒ–ç»“æœã€‚"
        )

        return (
            f"{retry_instruction}\n\n"
            f"ä¾›ä½ å‚è€ƒçš„ä¸Šä¸€è½®å›ç­”å¦‚ä¸‹ï¼ˆä»…ä¾›çº æ­£ï¼Œåˆ‡å‹¿ç…§æ¬ï¼‰ï¼š\n{previous_output}\n\n"
            f"ä»¥ä¸‹æ˜¯éœ€è¦é‡æ–°è¯„å®¡çš„ä»»åŠ¡è¯´æ˜ï¼š\n{base_prompt}"
        )
    
    def _build_revision_prompt(self, original: str, critiques: List[Dict], prompt_template: Optional[Dict] = None) -> str:
        feedback = "\n".join([
            f"è¯„å®¡å‘˜ {c.get('critic_name', 'N/A')}: {c.get('score', 0)}/12åˆ† "
            f"(å‡†ç¡®{c.get('accuracy', 0)} å®Œæ•´{c.get('completeness', 0)} "
            f"æ¸…æ™°{c.get('clarity', 0)} å®ç”¨{c.get('usefulness', 0)}) - {c.get('comment', 'N/A')}"
            for c in critiques
        ])
        
        if prompt_template and prompt_template.get('revision_prompt'):
            template = prompt_template['revision_prompt']
            return template.format(original=original, feedback=feedback)
        # é»˜è®¤æç¤ºè¯
        return f"""æ ¹æ®è¯„å®¡æ„è§æ”¹è¿›ä»¥ä¸‹ç­”æ¡ˆï¼Œåªè¾“å‡ºæ”¹è¿›åçš„å®Œæ•´ç­”æ¡ˆã€‚

åŸç­”æ¡ˆ:
{original}

è¯„å®¡æ„è§:
{feedback}

æ”¹è¿›è¦æ±‚: é’ˆå¯¹æ€§ä¿®å¤ç¼ºé™·ï¼Œè¡¥å……é—æ¼å†…å®¹ï¼Œä¼˜åŒ–è¡¨è¾¾ï¼Œå¢åŠ å®ç”¨æ€§ã€‚
è¾“å‡ºæ”¹è¿›åçš„ç­”æ¡ˆ:"""
    
    def _parse_critique(self, text: str, critic_name: str) -> Dict:
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ¨¡å‹è¿”å›çš„é”™è¯¯ä¿¡æ¯
        if text.strip().startswith("[Error:") or "Error code:" in text:
            print(f"\n{'='*60}")
            print(f"ğŸ” è§£æ {critic_name} çš„è¯„å®¡è¾“å‡º")
            print(f"{'='*60}")
            preview = text if len(text) < 800 else text[:800] + "..."
            print(f"åŸå§‹æ–‡æœ¬ ({len(text)} å­—ç¬¦):\n{preview}")
            print(f"{'='*60}\n")
            print(f"âš ï¸ æ£€æµ‹åˆ°æ¨¡å‹è¿”å›é”™è¯¯ï¼Œè¯¥æ¬¡è¯„å®¡å°†è¢«å¿½ç•¥ã€‚")
            print(f"{'='*60}\n")
            return {
                "critic_name": critic_name,
                "error": True,
                "raw_text": text,
                "comment": f"æ¨¡å‹è¿”å›é”™è¯¯: {text}",
                "score": 0, "accuracy": 0, "completeness": 0, "clarity": 0, "usefulness": 0
            }

        data = {
            "critic_name": critic_name,
            "accuracy": 0,
            "completeness": 0,
            "clarity": 0,
            "usefulness": 0,
            "score": 0,
            "comment": "",
            "missing_fields": [],
            "raw_text": text
        }

        print(f"\n{'='*60}")
        print(f"ğŸ” è§£æ {critic_name} çš„è¯„å®¡è¾“å‡º")
        print(f"{'='*60}")
        preview = text if len(text) < 800 else text[:800] + "..."
        print(f"åŸå§‹æ–‡æœ¬ ({len(text)} å­—ç¬¦):\n{preview}")
        print(f"{'='*60}\n")

        field_patterns = [
            ("accuracy", [r"å‡†ç¡®æ€§\s*[:ï¼š]\s*(\d+)", r"å‡†ç¡®æ€§\s*(\d+)", r"accuracy\s*[:ï¼š]?\s*(\d+)"]),
            ("completeness", [r"å®Œæ•´æ€§\s*[:ï¼š]\s*(\d+)", r"å®Œæ•´æ€§\s*(\d+)", r"completeness\s*[:ï¼š]?\s*(\d+)"]),
            ("clarity", [r"æ¸…æ™°æ€§\s*[:ï¼š]\s*(\d+)", r"æ¸…æ™°æ€§\s*(\d+)", r"clarity\s*[:ï¼š]?\s*(\d+)"]),
            ("usefulness", [r"å®ç”¨æ€§\s*[:ï¼š]\s*(\d+)", r"å®ç”¨æ€§\s*(\d+)", r"usefulness\s*[:ï¼š]?\s*(\d+)"])
        ]

        for field, patterns in field_patterns:
            found = False
            for pattern in patterns:
                if match := re.search(pattern, text, re.I):
                    data[field] = min(3, int(match.group(1)))
                    found = True
                    print(f"âœ“ æ‰¾åˆ°{field}: {data[field]}")
                    break
            if not found:
                data["missing_fields"].append(field)
                print(f"âœ— æœªæ‰¾åˆ°{field}è¯„åˆ†")

        total_patterns = [r"æ€»åˆ†\s*[:ï¼š]\s*(\d+)", r"æ€»åˆ†\s*(\d+)", r"total\s*[:ï¼š]?\s*(\d+)"]
        total_found = False
        for pattern in total_patterns:
            if match := re.search(pattern, text, re.I):
                data["score"] = min(12, int(match.group(1)))
                total_found = True
                print(f"âœ“ æ‰¾åˆ°æ€»åˆ†: {data['score']}")
                break

        if not total_found:
            data["score"] = data["accuracy"] + data["completeness"] + data["clarity"] + data["usefulness"]
            if 0 < data["score"] <= 12:
                print(
                    f"âœ“ è®¡ç®—æ€»åˆ†: {data['score']} = {data['accuracy']}+{data['completeness']}+"
                    f"{data['clarity']}+{data['usefulness']}"
                )
            else:
                data["missing_fields"].append("total")
                print("âœ— æ— æ³•ç¡®è®¤æ€»åˆ†")
        
        # æå–è¯„è¯­ - å¤šç§æ¨¡å¼å°è¯•
        comment_found = False
        
        # æ¨¡å¼1: æ ‡å‡†çš„"è¯„è¯­:"æ ¼å¼
        if comment := re.search(r"è¯„è¯­\s*[:ï¼š]\s*(.*?)(?:\n\n|\n(?:å‡†ç¡®æ€§|å®Œæ•´æ€§|æ¸…æ™°æ€§|å®ç”¨æ€§|æ€»åˆ†)|$)", text, re.I | re.DOTALL):
            comment_text = comment.group(1).strip()
            if comment_text and len(comment_text) > 5:  # ç¡®ä¿æœ‰å®è´¨å†…å®¹
                data["comment"] = comment_text
                comment_found = True
        
        # æ¨¡å¼2: æŸ¥æ‰¾"å»ºè®®"ã€"æ”¹è¿›"ã€"ç¼ºé™·"ç­‰å…³é”®è¯æ®µè½
        if not comment_found:
            keywords = [r"å»ºè®®[:ï¼š]?(.*?)(?:\n\n|$)", r"æ”¹è¿›[:ï¼š]?(.*?)(?:\n\n|$)", 
                       r"ç¼ºé™·[:ï¼š]?(.*?)(?:\n\n|$)", r"é—®é¢˜[:ï¼š]?(.*?)(?:\n\n|$)"]
            for pattern in keywords:
                if match := re.search(pattern, text, re.I | re.DOTALL):
                    comment_text = match.group(1).strip()
                    if comment_text and len(comment_text) > 5:
                        data["comment"] = comment_text
                        comment_found = True
                        break
        
        # æ¨¡å¼3: å¦‚æœå‰é¢éƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æå–æ€»åˆ†ä¹‹åçš„å†…å®¹
        if not comment_found:
            if after_score := re.search(r"æ€»åˆ†\s*[:ï¼š]\s*\d+\s*\n+(.*)", text, re.I | re.DOTALL):
                comment_text = after_score.group(1).strip()
                # ç§»é™¤å¯èƒ½çš„å¤šä½™æ¢è¡Œå’Œç©ºæ ¼
                comment_text = re.sub(r'\n{3,}', '\n\n', comment_text)
                if comment_text and len(comment_text) > 10:
                    data["comment"] = comment_text
                    comment_found = True
        
        # æ¨¡å¼4: å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œæå–æ‰€æœ‰è¯„åˆ†ä¹‹åçš„æ–‡æœ¬
        if not comment_found:
            # æ‰¾åˆ°æœ€åä¸€ä¸ªè¯„åˆ†é¡¹ä¹‹åçš„å†…å®¹
            last_score_pos = 0
            for pattern in [r"å‡†ç¡®æ€§\s*[:ï¼š]\s*\d+", r"å®Œæ•´æ€§\s*[:ï¼š]\s*\d+", 
                           r"æ¸…æ™°æ€§\s*[:ï¼š]\s*\d+", r"å®ç”¨æ€§\s*[:ï¼š]\s*\d+", r"æ€»åˆ†\s*[:ï¼š]\s*\d+"]:
                if match := re.search(pattern, text, re.I):
                    last_score_pos = max(last_score_pos, match.end())
            
            if last_score_pos > 0:
                remaining_text = text[last_score_pos:].strip()
                # ç§»é™¤"è¯„è¯­:"æ ‡ç­¾ï¼ˆå¦‚æœæœ‰ï¼‰
                remaining_text = re.sub(r'^è¯„è¯­\s*[:ï¼š]\s*', '', remaining_text, flags=re.I)
                if remaining_text and len(remaining_text) > 10:
                    data["comment"] = remaining_text
                    comment_found = True
        
        # å¦‚æœæ‰€æœ‰æ¨¡å¼éƒ½å¤±è´¥äº†ï¼Œä½¿ç”¨æ•´ä¸ªæ–‡æœ¬ä½œä¸ºè¯„è¯­ï¼ˆä½†æ’é™¤è¯„åˆ†è¡Œï¼‰
        if not comment_found or not data["comment"]:
            # ç§»é™¤æ‰€æœ‰è¯„åˆ†è¡Œ
            cleaned_text = re.sub(r'(å‡†ç¡®æ€§|å®Œæ•´æ€§|æ¸…æ™°æ€§|å®ç”¨æ€§|æ€»åˆ†)\s*[:ï¼š]\s*\d+\s*\n?', '', text, flags=re.I)
            cleaned_text = cleaned_text.strip()
            if cleaned_text and len(cleaned_text) > 15:
                data["comment"] = cleaned_text
            else:
                data["comment"] = f"æ¨¡å‹ {critic_name} æœªæŒ‰è¦æ±‚æä¾›è¯¦ç»†è¯„è¯­ã€‚"
                data["missing_fields"].append("comment")

        print(
            f"\næœ€ç»ˆè¯„åˆ†: å‡†ç¡®{data['accuracy']} å®Œæ•´{data['completeness']} "
            f"æ¸…æ™°{data['clarity']} å®ç”¨{data['usefulness']} = {data['score']}/12"
        )
        if data.get("missing_fields"):
            print(f"âš ï¸ ç¼ºå°‘å­—æ®µ: {data['missing_fields']}")
        print(f"{'='*60}\n")
        
        return data


